View(data_oroboros)
View(data_oroboros)
View(data_A)
View(data_input)
res_oro <- lapply(input_list, function(x) {
oroboros(data_input = x,
delta_interval = delta_interval,
routine_event = routine_event,
column_metric = column_metric)
})
path_to_files <- "/media/marcoantonio/KINGSTON/01. Excel 20D hyp OROBOROS/HS1.csv"
input_list <- lapply(path_to_files, function(x){
read.csv(x, head = TRUE, sep = ",", na.strings = "", check.names = FALSE, stringsAsFactors = FALSE)
})
names(input_list) <- basename(path_to_files)
cnms_list <- lapply(input_list, colnames)
col_eval <- diff(sapply(cnms_list, length))
if(any(col_eval!=0)) {
stop("Input files have different column length.")
}
cnms <- cnms_list[[1]]
cnms_chamber <- cnms[grep(pattern = "A:|B:", x = cnms, ignore.case = TRUE)]
cnms_spl <- sapply(strsplit(x = cnms_chamber, split = ": "), "[[", 2)
names(cnms_chamber) <- cnms_spl
metrics <- cnms_chamber[grep(pattern = "flux", x = cnms_chamber, ignore.case = TRUE)]
metrics <- which(cnms%in%metrics)
delta_interval = 20
routine_event = NULL
column_metric = metrics
IQR_cutoff = 1
res_oro <- lapply(input_list, function(x) {
oroboros(data_input = x,
delta_interval = delta_interval,
routine_event = routine_event,
column_metric = column_metric)
})
View(data_input)
data_oroboros[,c(1:5)]
column_metric
View(data_oroboros)
View(data_input)
View(data_oroboros)
View(data_input)
View(data_oroboros)
runApp('DeboraGuerrini_MarcoAntonio/Oroboros/Sunyata')
runApp('DeboraGuerrini_MarcoAntonio/Oroboros/Sunyata')
column_metric
runApp('DeboraGuerrini_MarcoAntonio/Oroboros/Sunyata')
runApp('DeboraGuerrini_MarcoAntonio/Oroboros/Sunyata')
path_to_files <- "/home/marcoantonio/Downloads/Fwd__oroboros_2/25.csv"
input_list <- lapply(path_to_files, function(x){
read.csv(x, head = TRUE, sep = ",", na.strings = "", check.names = FALSE, stringsAsFactors = FALSE)
})
names(input_list) <- basename(path_to_files)
cnms_list <- lapply(input_list, colnames)
col_eval <- diff(sapply(cnms_list, length))
if(any(col_eval!=0)) {
stop("Input files have different column length.")
}
cnms <- cnms_list[[1]]
cnms_chamber <- cnms[grep(pattern = "A:|B:", x = cnms, ignore.case = TRUE)]
cnms_spl <- sapply(strsplit(x = cnms_chamber, split = ": "), "[[", 2)
names(cnms_chamber) <- cnms_spl
metrics <- cnms_chamber[grep(pattern = "flux", x = cnms_chamber, ignore.case = TRUE)]
metrics <- which(cnms%in%metrics)
delta_interval = 20
routine_event = NULL
column_metric = metrics
IQR_cutoff = 1
res_oro <- lapply(input_list, function(x) {
oroboros(data_input = x,
delta_interval = delta_interval,
routine_event = routine_event,
column_metric = column_metric)
})
source("/mnt/hd/Marco/Dropbox/DeboraGuerrini_MarcoAntonio/Oroboros/CustomFunction/OroborosFunctions.R")
res_oro <- lapply(input_list, function(x) {
oroboros(data_input = x,
delta_interval = delta_interval,
routine_event = routine_event,
column_metric = column_metric)
})
debug(oroboros)
res_oro <- lapply(input_list, function(x) {
oroboros(data_input = x,
delta_interval = delta_interval,
routine_event = routine_event,
column_metric = column_metric)
})
View(data_A)
lapply(eventpoint_list, function(interval) {
stable_interval(einterval = interval,
data = data,
delta_interval = delta_interval,
IQR_cutoff = IQR_cutoff)
})
sapply(sinterval, function(x) {
metric <- x[[2]]
sum_dt <- summary(metric)
sum_dt <- c(sum_dt, sd = sd(metric), IQR = IQR(metric))
return(sum_dt)
})
View(sum_info)
chamber_int
View(data)
shiny::runApp('Captiomics/ASAP/Scripts/ASAP_v2')
2^6/53
2/50
#############################
# Data
#############################
files <- dir()
setwd("~/Desktop/Giovanna_meta_git/4_1_results_gabab_nocluster")
#############################
# Data
#############################
files <- dir()
files <- files[grep(pattern = "Giovanna_MetaAnalysis.txt", x = files)]
data_original <- read.delim(file = files, header = TRUE, sep = "\t", dec = ",")
View(data_original)
#############################
# Package Install/Load
#############################
## DOI: 10.1016/j.brs.2021.05.014
library(metafor)
library(dplyr)
library(metaviz)
library(ggplot2)
#############################
# Data
#############################
## Data.frame ##
### Columns description ###
#### - study: list of different studies
#### - brain_region: list of brain regions in each study
#### - n_c: number of samples in reference group for each study
#### - m_c: mean of samples in reference group for each study
#### - sd_c: standard deviation in reference group for each study
#### - n_t: number of samples in test group for each study
#### - m_t: mean of samples in test group for each study
#### - sd_t: standard deviation in test group for each study
#### - n_s: number of subjects in study
colnames(data_original) <- tolower(colnames(data_original))
data_split <- split(x = data_original, f = data_original$table.id)
resList <- lapply(data_split, function(data) {
#############################
# Calculate Effect Sizes
#############################
data_escalc <- escalc(n1i = n_t, m1i = m_t, sd1i = sd_t, n2i = n_c, m2i = m_c, sd2i = sd_c, measure = "SMD",
data = data, vtype = "UB", append = F)
# I use vtype = UB in my publications, it's more conservative
data <- cbind(data, data_escalc)
dup <- data$study[duplicated(data$study)]
data_dup <- data[data$study%in%dup,]
data_unique <- data[!data$study%in%dup,]
row.names(data_unique) <- data_unique$study
if(nrow(data_dup)>0) {
n_samples <- split(x = data_dup, f = data_dup$study)
n_samples <- lapply(n_samples, function(x) c(sum(x$n_c), sum(x$n_t)))
n_samples <- do.call("rbind", n_samples)
colnames(n_samples) <- c("n_c", "n_t")
n_samples <- rbind(n_samples, data_unique[,c("n_c","n_t")])
colnames(n_samples) <- c("n_samples_c", "n_samples_t")
} else {
n_samples <- data_unique[,c("n_c","n_t")]
colnames(n_samples) <- c("n_samples_c", "n_samples_t")
}
#############################
# Combined SMD (adjusted standardized mean difference by fixed-effect model)
#############################
studs <- unique(data_dup$study)
res <- lapply(studs, function(x, dt) {
ourdata <- dt[dt$study==x,]
z <- rma.uni(yi = yi, vi = vi, measure = "SMD",
data = ourdata, method = "FE")
c(z[1],x)
}, dt = data_dup)
data_comb <- data_dup[!duplicated(data_dup$study),]
row.names(data_comb) <- data_comb$study
for(s in studs) {
data_comb[s, "n_c"] <- max(data_comb[s, "n_c"])
data_comb[s, "n_t"] <- max(data_comb[s, "n_t"])
}
data_comb$yi <- as.numeric(lapply(res, '[[',1))
data_comb$vi <- ((data_comb$n_c + data_comb$n_t)/(data_comb$n_c * data_comb$n_t) +
data_comb$yi^2)/(2*(data_comb$n_c + data_comb$n_t))
# here you have to be consistent with the vtype above
# (you can find the formula corresponding to your outcome measure in the escalc function script,
# for SMD and vtype UB  it is: (n1+n2)/(n1*n2) +y^2 / (2*(n1+n2)) )
#############################
# Random-effect model
#############################
data_final <- rbind(data_unique, data_comb)
data_final$ci_low <- data_final$yi - (stats::qnorm(1 - (1 - 0.95)/2) * as.numeric(sqrt(data_final$vi)))
data_final$ci_high <- data_final$yi + (stats::qnorm(1 - (1 - 0.95)/2) * as.numeric(sqrt(data_final$vi)))
data_final <- cbind(data_final, n_samples)
row.names(data_final) <- data_final$id
data_final <- data_final[unique(data$id),]
res <- rma.uni(yi, vi, data = data_final, method="DL",
slab = paste(data_final$stud,
sep=""))
return(res)
})
data_split[[1]]
class(data_split[[1]])
data <- data_split[[1]]
#############################
# Calculate Effect Sizes
#############################
data_escalc <- escalc(n1i = n_t, m1i = m_t, sd1i = sd_t, n2i = n_c, m2i = m_c, sd2i = sd_c, measure = "SMD",
data = data, vtype = "UB", append = F)
data <- cbind(data, data_escalc)
dup <- data$study[duplicated(data$study)]
data_dup <- data[data$study%in%dup,]
data_unique <- data[!data$study%in%dup,]
row.names(data_unique) <- data_unique$study
if(nrow(data_dup)>0) {
n_samples <- split(x = data_dup, f = data_dup$study)
n_samples <- lapply(n_samples, function(x) c(sum(x$n_c), sum(x$n_t)))
n_samples <- do.call("rbind", n_samples)
colnames(n_samples) <- c("n_c", "n_t")
n_samples <- rbind(n_samples, data_unique[,c("n_c","n_t")])
colnames(n_samples) <- c("n_samples_c", "n_samples_t")
} else {
n_samples <- data_unique[,c("n_c","n_t")]
colnames(n_samples) <- c("n_samples_c", "n_samples_t")
}
#############################
# Combined SMD (adjusted standardized mean difference by fixed-effect model)
#############################
studs <- unique(data_dup$study)
res <- lapply(studs, function(x, dt) {
ourdata <- dt[dt$study==x,]
z <- rma.uni(yi = yi, vi = vi, measure = "SMD",
data = ourdata, method = "FE")
c(z[1],x)
}, dt = data_dup)
data_comb <- data_dup[!duplicated(data_dup$study),]
row.names(data_comb) <- data_comb$study
for(s in studs) {
data_comb[s, "n_c"] <- max(data_comb[s, "n_c"])
data_comb[s, "n_t"] <- max(data_comb[s, "n_t"])
}
data_comb$yi <- as.numeric(lapply(res, '[[',1))
data_comb$vi <- ((data_comb$n_c + data_comb$n_t)/(data_comb$n_c * data_comb$n_t) +
data_comb$yi^2)/(2*(data_comb$n_c + data_comb$n_t))
#############################
# Random-effect model
#############################
data_final <- rbind(data_unique, data_comb)
data_final$ci_low <- data_final$yi - (stats::qnorm(1 - (1 - 0.95)/2) * as.numeric(sqrt(data_final$vi)))
data_final$ci_high <- data_final$yi + (stats::qnorm(1 - (1 - 0.95)/2) * as.numeric(sqrt(data_final$vi)))
data_final <- cbind(data_final, n_samples)
row.names(data_final) <- data_final$id
data_final <- data_final[unique(data$id),]
rma.uni(yi, vi, data = data_final, method="DL",
slab = paste(data_final$stud,
sep=""))
View(data_final)
data_unique
data_comb
#############################
# Random-effect model
#############################
data_final <- rbind(data_unique, data_comb)
data_final
data_final$yi - (stats::qnorm(1 - (1 - 0.95)/2) * as.numeric(sqrt(data_final$vi)))
data_final$ci_low <- data_final$yi - (stats::qnorm(1 - (1 - 0.95)/2) * as.numeric(sqrt(data_final$vi)))
data_final$ci_high <- data_final$yi + (stats::qnorm(1 - (1 - 0.95)/2) * as.numeric(sqrt(data_final$vi)))
data_final
cbind(data_final, n_samples)
data_final <- cbind(data_final, n_samples)
data_final$id
data_final
row.names(data_final) <- data_final$id
data_final[unique(data$id),]
unique(data$id)
data$id
data
data_final
data_final[unique(data$id),]
unique(data$id)
data_original
data_original$id <- as.character(data_original$id)
data_split <- split(x = data_original, f = data_original$table.id)
resList <- lapply(data_split, function(data) {
#############################
# Calculate Effect Sizes
#############################
data_escalc <- escalc(n1i = n_t, m1i = m_t, sd1i = sd_t, n2i = n_c, m2i = m_c, sd2i = sd_c, measure = "SMD",
data = data, vtype = "UB", append = F)
# I use vtype = UB in my publications, it's more conservative
data <- cbind(data, data_escalc)
dup <- data$study[duplicated(data$study)]
data_dup <- data[data$study%in%dup,]
data_unique <- data[!data$study%in%dup,]
row.names(data_unique) <- data_unique$study
if(nrow(data_dup)>0) {
n_samples <- split(x = data_dup, f = data_dup$study)
n_samples <- lapply(n_samples, function(x) c(sum(x$n_c), sum(x$n_t)))
n_samples <- do.call("rbind", n_samples)
colnames(n_samples) <- c("n_c", "n_t")
n_samples <- rbind(n_samples, data_unique[,c("n_c","n_t")])
colnames(n_samples) <- c("n_samples_c", "n_samples_t")
} else {
n_samples <- data_unique[,c("n_c","n_t")]
colnames(n_samples) <- c("n_samples_c", "n_samples_t")
}
#############################
# Combined SMD (adjusted standardized mean difference by fixed-effect model)
#############################
studs <- unique(data_dup$study)
res <- lapply(studs, function(x, dt) {
ourdata <- dt[dt$study==x,]
z <- rma.uni(yi = yi, vi = vi, measure = "SMD",
data = ourdata, method = "FE")
c(z[1],x)
}, dt = data_dup)
data_comb <- data_dup[!duplicated(data_dup$study),]
row.names(data_comb) <- data_comb$study
for(s in studs) {
data_comb[s, "n_c"] <- max(data_comb[s, "n_c"])
data_comb[s, "n_t"] <- max(data_comb[s, "n_t"])
}
data_comb$yi <- as.numeric(lapply(res, '[[',1))
data_comb$vi <- ((data_comb$n_c + data_comb$n_t)/(data_comb$n_c * data_comb$n_t) +
data_comb$yi^2)/(2*(data_comb$n_c + data_comb$n_t))
# here you have to be consistent with the vtype above
# (you can find the formula corresponding to your outcome measure in the escalc function script,
# for SMD and vtype UB  it is: (n1+n2)/(n1*n2) +y^2 / (2*(n1+n2)) )
#############################
# Random-effect model
#############################
data_final <- rbind(data_unique, data_comb)
data_final$ci_low <- data_final$yi - (stats::qnorm(1 - (1 - 0.95)/2) * as.numeric(sqrt(data_final$vi)))
data_final$ci_high <- data_final$yi + (stats::qnorm(1 - (1 - 0.95)/2) * as.numeric(sqrt(data_final$vi)))
data_final <- cbind(data_final, n_samples)
row.names(data_final) <- data_final$id
data_final <- data_final[unique(data$id),]
return(data_final)
})
resList[[1]]
dirname(getwd())
getwd()
#############################
# Data
#############################
files <- dir()
files <- files[grep(pattern = "Giovanna_MetaAnalysis.txt", x = files)]
data_original <- read.delim(file = files, header = TRUE, sep = "\t", dec = ",")
source(file = paste(getwd(), "0_2_MetaAnalysis_code.R", sep = "/"))
resData <- do.call("rbind",resList)
View(resData)
resList <- lapply(data_split, function(data) {
#############################
# Calculate Effect Sizes
#############################
data_escalc <- escalc(n1i = n_t, m1i = m_t, sd1i = sd_t, n2i = n_c, m2i = m_c, sd2i = sd_c, measure = "SMD",
data = data, vtype = "UB", append = F)
# I use vtype = UB in my publications, it's more conservative
data <- cbind(data, data_escalc)
dup <- data$study[duplicated(data$study)]
data_dup <- data[data$study%in%dup,]
data_unique <- data[!data$study%in%dup,]
row.names(data_unique) <- data_unique$study
if(nrow(data_dup)>0) {
n_samples <- split(x = data_dup, f = data_dup$study)
n_samples <- lapply(n_samples, function(x) c(sum(x$n_c), sum(x$n_t)))
n_samples <- do.call("rbind", n_samples)
colnames(n_samples) <- c("n_c", "n_t")
n_samples <- rbind(n_samples, data_unique[,c("n_c","n_t")])
colnames(n_samples) <- c("n_samples_c", "n_samples_t")
} else {
n_samples <- data_unique[,c("n_c","n_t")]
colnames(n_samples) <- c("n_samples_c", "n_samples_t")
}
#############################
# Combined SMD (adjusted standardized mean difference by fixed-effect model)
#############################
studs <- unique(data_dup$study)
res <- lapply(studs, function(x, dt) {
ourdata <- dt[dt$study==x,]
z <- rma.uni(yi = yi, vi = vi, measure = "SMD",
data = ourdata, method = "FE")
c(z[1],x)
}, dt = data_dup)
data_comb <- data_dup[!duplicated(data_dup$study),]
row.names(data_comb) <- data_comb$study
for(s in studs) {
data_comb[s, "n_c"] <- max(data_comb[s, "n_c"])
data_comb[s, "n_t"] <- max(data_comb[s, "n_t"])
}
data_comb$yi <- as.numeric(lapply(res, '[[',1))
data_comb$vi <- ((data_comb$n_c + data_comb$n_t)/(data_comb$n_c * data_comb$n_t) +
data_comb$yi^2)/(2*(data_comb$n_c + data_comb$n_t))
# here you have to be consistent with the vtype above
# (you can find the formula corresponding to your outcome measure in the escalc function script,
# for SMD and vtype UB  it is: (n1+n2)/(n1*n2) +y^2 / (2*(n1+n2)) )
#############################
#
#############################
data_final <- rbind(data_unique, data_comb)
data_final$ci_low <- data_final$yi - (stats::qnorm(1 - (1 - 0.95)/2) * as.numeric(sqrt(data_final$vi)))
data_final$ci_high <- data_final$yi + (stats::qnorm(1 - (1 - 0.95)/2) * as.numeric(sqrt(data_final$vi)))
row.names(data_final) <- data_final$id
data_final <- data_final[unique(data$id),]
return(data_final)
})
resData <- do.call("rbind",resList)
View(resData)
setwd("~/Desktop/Giovanna_meta_git/3_3_results_sensanalysis_wobias")
#############################
# Data
#############################
files <- dir()
files <- files[grep(pattern = "Giovanna_MetaAnalysis.txt", x = files)]
data_original <- read.delim(file = files, header = TRUE, sep = "\t", dec = ",")
### Columns description ###
#### - study: list of different studies
#### - brain_region: list of brain regions in each study
#### - n_c: number of samples in reference group for each study
#### - m_c: mean of samples in reference group for each study
#### - sd_c: standard deviation in reference group for each study
#### - n_t: number of samples in test group for each study
#### - m_t: mean of samples in test group for each study
#### - sd_t: standard deviation in test group for each study
#### - n_s: number of subjects in study
colnames(data_original) <- tolower(colnames(data_original))
data_original$id <- as.character(data_original$id)
data_split <- split(x = data_original, f = data_original$table.id)
resList <- lapply(data_split, function(data) {
#############################
# Calculate Effect Sizes
#############################
data_escalc <- escalc(n1i = n_t, m1i = m_t, sd1i = sd_t, n2i = n_c, m2i = m_c, sd2i = sd_c, measure = "SMD",
data = data, vtype = "UB", append = F)
# I use vtype = UB in my publications, it's more conservative
data <- cbind(data, data_escalc)
dup <- data$study[duplicated(data$study)]
data_dup <- data[data$study%in%dup,]
data_unique <- data[!data$study%in%dup,]
row.names(data_unique) <- data_unique$study
if(nrow(data_dup)>0) {
n_samples <- split(x = data_dup, f = data_dup$study)
n_samples <- lapply(n_samples, function(x) c(sum(x$n_c), sum(x$n_t)))
n_samples <- do.call("rbind", n_samples)
colnames(n_samples) <- c("n_c", "n_t")
n_samples <- rbind(n_samples, data_unique[,c("n_c","n_t")])
colnames(n_samples) <- c("n_samples_c", "n_samples_t")
} else {
n_samples <- data_unique[,c("n_c","n_t")]
colnames(n_samples) <- c("n_samples_c", "n_samples_t")
}
#############################
# Combined SMD (adjusted standardized mean difference by fixed-effect model)
#############################
studs <- unique(data_dup$study)
res <- lapply(studs, function(x, dt) {
ourdata <- dt[dt$study==x,]
z <- rma.uni(yi = yi, vi = vi, measure = "SMD",
data = ourdata, method = "FE")
c(z[1],x)
}, dt = data_dup)
data_comb <- data_dup[!duplicated(data_dup$study),]
row.names(data_comb) <- data_comb$study
for(s in studs) {
data_comb[s, "n_c"] <- max(data_comb[s, "n_c"])
data_comb[s, "n_t"] <- max(data_comb[s, "n_t"])
}
data_comb$yi <- as.numeric(lapply(res, '[[',1))
data_comb$vi <- ((data_comb$n_c + data_comb$n_t)/(data_comb$n_c * data_comb$n_t) +
data_comb$yi^2)/(2*(data_comb$n_c + data_comb$n_t))
# here you have to be consistent with the vtype above
# (you can find the formula corresponding to your outcome measure in the escalc function script,
# for SMD and vtype UB  it is: (n1+n2)/(n1*n2) +y^2 / (2*(n1+n2)) )
#############################
#
#############################
data_final <- rbind(data_unique, data_comb)
data_final$ci_low <- data_final$yi - (stats::qnorm(1 - (1 - 0.95)/2) * as.numeric(sqrt(data_final$vi)))
data_final$ci_high <- data_final$yi + (stats::qnorm(1 - (1 - 0.95)/2) * as.numeric(sqrt(data_final$vi)))
row.names(data_final) <- data_final$id
data_final <- data_final[unique(data$id),]
return(data_final)
})
resData <- do.call("rbind",resList)
View(resData)
#############################
# Data
#############################
files <- dir()
files <- files[grep(pattern = "Giovanna_MetaAnalysis.txt", x = files)]
data_original <- read.delim(file = files, header = TRUE, sep = "\t", dec = ",")
#############################
# Data
#############################
files <- dir()
files <- files[grep(pattern = "Giovanna_MetaAnalysis.txt", x = files)]
data_original <- read.delim(file = files, header = TRUE, sep = "\t", dec = ",")
#############################
# Save
#############################
write.table(x = resData, file = "4_1_results_gabab_nocluster.txt", sep = "\t", row.names = FALSE)
getwd()
setwd("~/Desktop/Giovanna_meta_git/4_1_results_gabab_nocluster")
#############################
# Data
#############################
files <- dir()
files <- files[grep(pattern = "Giovanna_MetaAnalysis.txt", x = files)]
data_original <- read.delim(file = files, header = TRUE, sep = "\t", dec = ",")
source(file = paste(getwd(), "0_2_MetaAnalysis_code.R", sep = "/"))
